{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from collections import Counter\n",
    "from general_functions import SENT_BEG, SENT_END\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Read text data\n",
    "* Preprocess and clean the text\n",
    "* Save clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(filepath) -> str:\n",
    "    \"\"\"Read a text file using UTF-8 encoding\n",
    "    Parameters:\n",
    "        filepath (str): the path to the file\n",
    "    Returns:\n",
    "        str: the file contents\"\"\"\n",
    "    \n",
    "    text = None\n",
    "    \n",
    "    with open(filepath, 'r', encoding = 'utf-8-sig') as f:  # utf-8-sig removes the byte order maker\n",
    "        text = f.read()\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text) -> (list, Counter):\n",
    "    \"\"\"Preprocess text and count tokens\n",
    "    Parameters:\n",
    "        text (str): the text to process\n",
    "    Returns:\n",
    "        list: a list of tokens in each sentence\n",
    "        Counter: token counts\"\"\"\n",
    "    \n",
    "    token_counter = Counter()\n",
    "    \n",
    "    text = text.replace('\\n', ' ').replace('_', '')  # replace newline char with whitespace, and remove underscore\n",
    "    \n",
    "    # split text into sentences\n",
    "    text = sent_tokenize(text)\n",
    "    print('Number of sentences:', len(text))\n",
    "    \n",
    "    # remove all punctuation and tokenise\n",
    "    punct_remover = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        sentence = text[i].lower()  # convert to lower case\n",
    "        \n",
    "        # remove punctuation, add start-of-sent and end-of-sent chars\n",
    "        tokens = punct_remover.tokenize(sentence)\n",
    "        tokens.insert(0, SENT_BEG)\n",
    "        tokens.append(SENT_END)\n",
    "        \n",
    "        token_counter.update(tokens)  # count tokens\n",
    "        \n",
    "        text[i] = tokens  # overwrite sentence with list of processed tokens\n",
    "        \n",
    "    # vocab stats\n",
    "    print('Vocab size |V|:', len(token_counter))\n",
    "    print('Total words excluding {} and {}:'.format(SENT_BEG, SENT_END), sum(token_counter.values()) - token_counter[SENT_BEG] - token_counter[SENT_END])\n",
    "        \n",
    "    return text, token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_pickle(filepath, obj) -> None:\n",
    "    \"\"\"Write an object to a pickle file\n",
    "    Parameters:\n",
    "        filepath (str): path to the pickle file\n",
    "        obj (Any): object to write\n",
    "    Returns:\n",
    "        None\"\"\"\n",
    "    \n",
    "    with open(filepath, 'wb') as pckl:\n",
    "        pickle.dump(obj, pckl, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    pckl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Twenty Thousand Leagues Under the Sea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 6587\n",
      "Vocab size |V|: 8669\n",
      "Total words excluding <s> and </s>: 104174\n"
     ]
    }
   ],
   "source": [
    "ttl_sents = read_text('./Corpus/TwentyThousandLeagues.txt')\n",
    "\n",
    "ttl_sents, ttl_counts = preprocess(ttl_sents)\n",
    "\n",
    "write_to_pickle('./Pickles/ttl_sents.pickle', ttl_sents)\n",
    "write_to_pickle('./Pickles/ttl_counts.pickle', ttl_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Around the World in Eighty Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 2877\n",
      "Vocab size |V|: 6829\n",
      "Total words excluding <s> and </s>: 64343\n"
     ]
    }
   ],
   "source": [
    "atw_sents = read_text('./Corpus/AroundTheWorld.txt')\n",
    "\n",
    "atw_sents, atw_counts = preprocess(atw_sents)\n",
    "\n",
    "write_to_pickle('./Pickles/atw_sents.pickle', atw_sents)\n",
    "write_to_pickle('./Pickles/atw_counts.pickle', atw_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comined corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 9464\n",
      "Vocab size |V|: 11553\n",
      "Total words excluding <s> and </s>: 168517\n"
     ]
    }
   ],
   "source": [
    "cor_sents = []\n",
    "\n",
    "cor_sents.extend(ttl_sents)\n",
    "cor_sents.extend(atw_sents)\n",
    "shuffle(cor_sents)\n",
    "\n",
    "cor_counts = ttl_counts + atw_counts\n",
    "\n",
    "print('Number of sentences:', len(cor_sents))\n",
    "print('Vocab size |V|:', len(cor_counts))\n",
    "print('Total words excluding {} and {}:'.format(SENT_BEG, SENT_END), sum(cor_counts.values()) - cor_counts[SENT_BEG] - cor_counts[SENT_END])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_pickle('./Pickles/cor_sents.pickle', cor_sents)\n",
    "write_to_pickle('./Pickles/cor_counts.pickle', cor_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
